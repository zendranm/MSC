{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAEGAN_v1.ipynb","provenance":[],"collapsed_sections":["CzPpqjB4XJMW","uyYCnVSULd2b","ETPBdG_fWTJE","cTKd3hAZrOUf"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CzPpqjB4XJMW","colab_type":"text"},"source":["# Drive and GPU Connection"]},{"cell_type":"markdown","metadata":{"id":"7yDVpNGie-yB","colab_type":"text"},"source":["Run to connect with google drive to access data"]},{"cell_type":"code","metadata":{"id":"_CUcVRIaXOe6","colab_type":"code","outputId":"bec62481-da92-4e2f-810a-805e8d754baf","executionInfo":{"status":"ok","timestamp":1591691271586,"user_tz":-120,"elapsed":21206,"user":{"displayName":"Michał Zendran","photoUrl":"https://lh4.googleusercontent.com/-EdIWItliYF8/AAAAAAAAAAI/AAAAAAAAGOc/mlyaLViln_w/s64/photo.jpg","userId":"03602240860466142034"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4bbPYYBUgTmz","colab_type":"text"},"source":["Verify GPU"]},{"cell_type":"code","metadata":{"id":"YYRUNyj5gU8C","colab_type":"code","colab":{}},"source":["# import tensorflow as tf \n","# tf.test.gpu_device_name()\n","\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvBjm2Sfids0","colab_type":"code","colab":{}},"source":["!nvcc --version"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uyYCnVSULd2b","colab_type":"text"},"source":["# Data preprocesing and visualization"]},{"cell_type":"code","metadata":{"id":"7EIX3LfuLcmx","colab_type":"code","outputId":"dc06b05b-1631-4fad-ab49-6e93a5392dfc","executionInfo":{"status":"ok","timestamp":1591708557641,"user_tz":-120,"elapsed":4785,"user":{"displayName":"Michał Zendran","photoUrl":"https://lh4.googleusercontent.com/-EdIWItliYF8/AAAAAAAAAAI/AAAAAAAAGOc/mlyaLViln_w/s64/photo.jpg","userId":"03602240860466142034"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras import Sequential, Input, Model\n","from tensorflow.keras.layers import Dense, Activation, Flatten, Reshape, Lambda\n","from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, MaxPooling2D\n","from tensorflow.keras.layers import LeakyReLU, Dropout\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from keras import backend as K\n","\n","def load_data(filename):\n","    with np.load(filename) as f:\n","        x_train, x_test = f['arr_0'], f['arr_1']\n","        y_train, y_test = f['arr_2'], f['arr_3']\n","        return (x_train, y_train), (x_test, y_test)\n","\n","def preprocess_data(data, feature_range=(-1.0, 1.0)):\n","    (x_train, y_train), (x_test, y_test) = data\n","    min, max = feature_range\n","\n","    x_train = x_train.reshape(-1, 160, 160, 3).astype(np.float32)\n","    x_train = x_train/255.0\n","    x_train = x_train * (max - min) + min\n","\n","    x_test = x_test.reshape(-1, 160, 160, 3).astype(np.float32)\n","    x_test = x_test/255.0\n","    x_test = x_test * (max - min) + min\n","\n","    y_train = y_train.reshape(-1, 160, 160, 3).astype(np.float32)\n","    y_train = y_train/255.0\n","    y_train = y_train * (max - min) + min\n","\n","    y_test = y_test.reshape(-1, 160, 160, 3).astype(np.float32)\n","    y_test = y_test/255.0\n","    y_test = y_test * (max - min) + min\n","\n","    print(\"Values after preprocessing, min: \" + str(x_train.min()) + \", max: \" + str(x_train.max()))\n","    return (x_train, y_train), (x_test, y_test)\n","\n","def add_mirrored_data(data):\n","    (x_train, y_train), (x_test, y_test) = data\n","\n","    x_train_mirrored = np.flip(x_train, 2)\n","    y_train_mirrored = np.flip(y_train, 2)\n","    x_test_mirrored = np.flip(x_test, 2)\n","    y_test_mirrored = np.flip(y_test, 2)\n","\n","    x_train = np.concatenate((x_train, x_train_mirrored))\n","    y_train = np.concatenate((y_train, y_train_mirrored))\n","    x_test = np.concatenate((x_test, x_test_mirrored))\n","    y_test = np.concatenate((y_test, y_test_mirrored))\n","\n","    return (x_train, y_train), (x_test, y_test)\n","\n","def get_random_batch(data, batch_size):\n","    idx = np.random.choice(data.shape[0], batch_size)\n","    batch = data[idx].reshape((batch_size, 160, 160, 3))\n","    return batch\n","\n","def show_images(images, feature_range=(-1.0, 1.0)):\n","    min, max = feature_range\n","    plt.figure(figsize=(10,10))\n","    \n","    for im in range(images.shape[0]):\n","      plt.subplot(4, 4, im+1)\n","      image = images[im, :, :, :]\n","      image = np.reshape(image, [160,160,3])\n","      image = (image - min) / (max - min)\n","      plt.imshow(image)\n","      plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","def visualize(sample_X, sample_Y, encoder, decoder, feature_range=(-1.0, 1.0)):\n","    min, max = feature_range\n","    count = sample_X.shape[0]\n","\n","    reconstructed_X = []\n","    reconstructed_Y = []\n","    accumulated = []\n","\n","    for n in range(count):\n","        code = encoder.predict(sample_X[n][None])[0]\n","        reco = decoder.predict(code[None])[0]\n","\n","        reconstructed_X.append(reco)\n","\n","        code = encoder.predict(sample_Y[n][None])[0]\n","        reco = decoder.predict(code[None])[0]\n","\n","        reconstructed_Y.append(reco)\n","\n","    for i in range(4):\n","        accumulated.append(sample_X[i])\n","        accumulated.append(reconstructed_X[i])\n","        accumulated.append(sample_Y[i])\n","        accumulated.append(reconstructed_Y[i])\n","\n","    count = len(accumulated)\n","\n","    plt.figure(figsize=(8,8))\n","\n","    for i in range(count):\n","        plt.subplot(4, 4, i + 1)\n","        image = (accumulated[i] - min) / (max - min)\n","        plt.imshow(image)\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ETPBdG_fWTJE","colab_type":"text"},"source":["# VAEGAN models"]},{"cell_type":"code","metadata":{"id":"Iv2juI9uWXcV","colab_type":"code","colab":{}},"source":["def sampling(args):\n","    mean_mu, log_var = args\n","    epsilon = tf.random.normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n","    return mean_mu + K.exp(log_var/2)*epsilon\n","\n","def build_encoder(img_shape, code_size):\n","    encoder_input = Input(shape=img_shape, name='encoder_input')\n","\n","    x = encoder_input\n","\n","    x = Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2), padding='same', name='encoder_conv_1')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2), padding='same', name='encoder_conv_2')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2), padding='same', name='encoder_conv_3')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Flatten()(x)\n","\n","    mean_mu = Dense(code_size, name='mean_mu')(x)\n","    log_var = Dense(code_size, name='log_var')(x)\n","    encoder_output = Lambda(sampling, name='encoder_output')([mean_mu, log_var])\n","\n","    return mean_mu, log_var, Model(encoder_input, encoder_output)\n","\n","def build_generator(img_shape, code_size):\n","    size1 = int(img_shape[0]/8) # 8 is 2^number_of_UpSampling2D_layers in generator\n","    size2 = int(img_shape[1]/8)\n","\n","    generator_input = Input(shape=(int(code_size,)), name='generator_input')\n","    x = generator_input\n","\n","    x = Dense(units=(size1*size2*256))(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = Reshape(target_shape=(size1, size2, 256))(x)\n","\n","    x = Conv2DTranspose(filters=256, kernel_size=(5, 5), strides=(1, 1), padding='same', name='generator_conv_1')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2DTranspose(filters=128, kernel_size=(5, 5), strides=(2, 2), padding='same', name='generator_conv_2')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2DTranspose(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name='generator_conv_3')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2DTranspose(filters=3, kernel_size=(5, 5), strides=(2, 2), padding='same', name='generator_conv_4')(x)\n","    x = Activation('tanh')(x)\n","\n","    generator_output = x\n","\n","    model = Model(generator_input, generator_output)\n","\n","    return model\n","\n","def build_discriminator(img_shape):\n","\n","    discriminator_input = Input(shape=img_shape, name='discriminator_input')\n","    x = discriminator_input\n","\n","    x = Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), padding='same', name='discriminator_conv_0')(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2), padding='same', name='discriminator_conv_1')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2), padding='same', name='discriminator_conv_2')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2), padding='same', name='discriminator_conv_3')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Flatten()(x)\n","    x = Dense(1, activation='sigmoid')(x)\n","\n","    discriminator_output = x\n","\n","    model = Model(discriminator_input, discriminator_output)\n","\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nO6s4S5W3NO","colab_type":"text"},"source":["# Training XY"]},{"cell_type":"code","metadata":{"id":"OyD_5tRabmiC","colab_type":"code","outputId":"ebc6dcd6-2f2b-42f1-b5ac-4309a377a30c","executionInfo":{"status":"error","timestamp":1591697188916,"user_tz":-120,"elapsed":927916,"user":{"displayName":"Michał Zendran","photoUrl":"https://lh4.googleusercontent.com/-EdIWItliYF8/AAAAAAAAAAI/AAAAAAAAGOc/mlyaLViln_w/s64/photo.jpg","userId":"03602240860466142034"}},"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1jdEqgGyIBU5XSI4qPAivXdpY_1SDQH43"}},"source":["def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","def encoder_loss(original, faked):\n","    tf.debugging.check_numerics(faked, \"Nan or Inf in input occurred\", name=None)\n","    return mean_squared_error(original, faked)\n","\n","def kl_loss(mean_mu, log_var):\n","    kl_loss =  -0.5 * tf.reduce_sum(1 + log_var - tf.square(mean_mu) - tf.exp(log_var), axis = 1)\n","    return kl_loss\n","\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","mean_squared_error = tf.keras.losses.MeanSquaredError()\n","\n","# Load and prepare data\n","npz_path = './drive/My Drive/ColabStuff/DiCaprioToDowneyJr_Big_VAE.npz'\n","(x_train, y_train), (x_test, y_test) = load_data(npz_path)\n","\n","value_range = (-1.0, 1.0)\n","print(\"Imported data shape: \" + str(x_train.shape))\n","\n","(x_train, y_train), (x_test, y_test) = preprocess_data(((x_train, y_train), (x_test, y_test)), value_range)\n","\n","# imported_data_sample = get_random_batch(x_train, 16)\n","# show_images(imported_data_sample, value_range)\n","\n","(x_train, y_train), (x_test, y_test) = add_mirrored_data(((x_train, y_train), (x_test, y_test)))\n","\n","# imported_data_sample = get_random_batch(x_train, 16)\n","# show_images(imported_data_sample, value_range)\n","\n","save_path = './drive/My Drive/ColabStuff/VAEGAN/'\n","\n","train_x = np.concatenate((x_train, x_test))\n","train_y = np.concatenate((y_train, y_test))\n","train_z = np.concatenate((train_x, train_y))\n","\n","batch_size = 32\n","code_size = 200\n","epochs_number_1 =20000\n","epochs_number_2 =500\n","LEARNING_RATE = 0.0005\n","IMG_SHAPE = train_x.shape[1:]\n","\n","enc_loss_plot = []\n","gen_loss_plot = []\n","disc_loss_plot = []\n","plot_iteration = []\n","\n","mean, logvar, encoder = build_encoder(IMG_SHAPE, code_size)\n","discriminator = build_discriminator(IMG_SHAPE)\n","generator = build_generator(IMG_SHAPE, code_size)\n","\n","encoder_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n","generator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n","discriminator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n","\n","for i in range(1, epochs_number_1 + 1):\n","  images_train = get_random_batch(train_z, batch_size)\n","\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as enc_tape:\n","      latent = encoder(images_train)\n","      generated_images = generator(latent, training=True)\n","\n","      real_output = discriminator(images_train, training=True)\n","      fake_output = discriminator(generated_images, training=True)\n","\n","      enc_loss = encoder_loss(images_train, generated_images) * 100\n","      gen_loss = generator_loss(fake_output) + enc_loss\n","      disc_loss = discriminator_loss(real_output, fake_output)\n","  \n","  gradients_of_encoder = enc_tape.gradient(enc_loss, encoder.trainable_variables)\n","  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","  encoder_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n","  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","  enc_loss_plot.append(enc_loss)\n","  gen_loss_plot.append(gen_loss)\n","  disc_loss_plot.append(disc_loss)\n","  plot_iteration.append(i)\n","  \n","  if (i != 1) and (i % 10 == 0):\n","    print('setp: ' + str(i) + \", gen_loss: \" + str(gen_loss.numpy()) + \", disc_loss: \" + str(disc_loss.numpy()) + \", enc_loss: \" + str(enc_loss.numpy()))\n","\n","  if (i != 1)and (i % 100 == 0):\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, enc_loss_plot, label=\"enccoder loss\")\n","      ax1.legend()\n","\n","      ax1.plot(plot_iteration, gen_loss_plot, \"C1\", label=\"generator loss\")\n","      ax1.legend()\n","\n","      ax1.plot(plot_iteration, disc_loss_plot, \"C2\", label=\"discriminator loss\")\n","      ax1.legend()\n","\n","      plt.show()\n","  \n","  if (i != 1) and (i % 100 == 0):\n","    test_sample_X = get_random_batch(train_x, 4)\n","    test_sample_Y = get_random_batch(train_y, 4)\n","\n","    visualize(test_sample_X, test_sample_Y, encoder, generator, value_range)\n","\n","  if (i != 1) and (i % 100 == 0):\n","    encoder.save(save_path + \"encoder_\" + str(i) + \".h5\")\n","    generator.save(save_path + \"generator_XY_\" + str(i) + \".h5\")\n","    discriminator.save(save_path + \"discriminator_XY_\" + str(i) + \".h5\")\n","\n","print(\"Program finished successfully!\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"cTKd3hAZrOUf","colab_type":"text"},"source":["#Training X"]},{"cell_type":"code","metadata":{"id":"vzWN_ux8rTGl","colab_type":"code","outputId":"93af9643-4e01-4828-f420-a222068fcc70","executionInfo":{"status":"error","timestamp":1591707609779,"user_tz":-120,"elapsed":4650635,"user":{"displayName":"Michał Zendran","photoUrl":"https://lh4.googleusercontent.com/-EdIWItliYF8/AAAAAAAAAAI/AAAAAAAAGOc/mlyaLViln_w/s64/photo.jpg","userId":"03602240860466142034"}},"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1Yzlv36xvHbBiGwixUcJXb0oXxdN1EBPg"}},"source":["# Load model\n","encoder_path = './drive/My Drive/ColabStuff/VAEGAN/encoder_200.h5'\n","loaded_enocder = tf.keras.models.load_model(encoder_path, compile=False)\n","\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","def encoder_loss(original, faked):\n","    tf.debugging.check_numerics(faked, \"Nan or Inf in input occurred\", name=None)\n","    return mean_squared_error(original, faked)\n","\n","def kl_loss(mean_mu, log_var):\n","    kl_loss =  -0.5 * tf.reduce_sum(1 + log_var - tf.square(mean_mu) - tf.exp(log_var), axis = 1)\n","    return kl_loss\n","\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","mean_squared_error = tf.keras.losses.MeanSquaredError()\n","\n","# Load and prepare data\n","npz_path = './drive/My Drive/ColabStuff/DiCaprioToDowneyJr_Big_VAE.npz'\n","(x_train, y_train), (x_test, y_test) = load_data(npz_path)\n","\n","value_range = (-1.0, 1.0)\n","print(\"Imported data shape: \" + str(x_train.shape))\n","\n","(x_train, y_train), (x_test, y_test) = preprocess_data(((x_train, y_train), (x_test, y_test)), value_range)\n","\n","# imported_data_sample = get_random_batch(x_train, 16)\n","# show_images(imported_data_sample, value_range)\n","\n","(x_train, y_train), (x_test, y_test) = add_mirrored_data(((x_train, y_train), (x_test, y_test)))\n","\n","# imported_data_sample = get_random_batch(x_train, 16)\n","# show_images(imported_data_sample, value_range)\n","\n","save_path = './drive/My Drive/ColabStuff/VAEGAN/'\n","\n","train_x = np.concatenate((x_train, x_test))\n","train_y = np.concatenate((y_train, y_test))\n","train_z = np.concatenate((train_x, train_y))\n","\n","batch_size = 32\n","code_size = 200\n","epochs_number_1 =20000\n","epochs_number_2 =500\n","LEARNING_RATE = 0.0005\n","IMG_SHAPE = train_x.shape[1:]\n","\n","gen_loss_plot = []\n","disc_loss_plot = []\n","plot_iteration = []\n","\n","discriminator = build_discriminator(IMG_SHAPE)\n","generator = build_generator(IMG_SHAPE, code_size)\n","\n","generator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n","discriminator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n","\n","for i in range(1, epochs_number_1 + 1):\n","  images_train = get_random_batch(train_x, batch_size)\n","  latent = loaded_enocder.predict(images_train)\n","\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","      generated_images = generator(latent, training=True)\n","\n","      real_output = discriminator(images_train, training=True)\n","      fake_output = discriminator(generated_images, training=True)\n","\n","      enc_loss = encoder_loss(images_train, generated_images) * 100\n","      gen_loss = generator_loss(fake_output) + enc_loss\n","      disc_loss = discriminator_loss(real_output, fake_output)\n","  \n","  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","  gen_loss_plot.append(gen_loss)\n","  disc_loss_plot.append(disc_loss)\n","  plot_iteration.append(i)\n","  \n","  if (i != 1) and (i % 10 == 0):\n","    print('setp: ' + str(i) + \", gen_loss: \" + str(gen_loss.numpy()) + \", disc_loss: \" + str(disc_loss.numpy()))\n","\n","  if (i != 1)and (i % 100 == 0):\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, gen_loss_plot, \"C1\", label=\"generator loss\")\n","      ax1.legend()\n","\n","      ax1.plot(plot_iteration, disc_loss_plot, \"C2\", label=\"discriminator loss\")\n","      ax1.legend()\n","\n","      plt.show()\n","  \n","  if (i != 1) and (i % 100 == 0):\n","    test_sample_X = get_random_batch(train_x, 4)\n","    test_sample_Y = get_random_batch(train_y, 4)\n","\n","    visualize(test_sample_X, test_sample_Y, loaded_enocder, generator, value_range)\n","\n","  if (i != 1) and (i % 100 == 0):\n","    generator.save(save_path + \"generator_X_\" + str(i) + \".h5\")\n","    discriminator.save(save_path + \"discriminator_X_\" + str(i) + \".h5\")\n","\n","print(\"Program finished successfully!\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"cKs8piD0N-jM","colab_type":"text"},"source":["# Training Y"]},{"cell_type":"code","metadata":{"id":"0dDugu3JOB7r","colab_type":"code","outputId":"7a3fe225-530c-4c32-986a-e0bef406fdd0","executionInfo":{"status":"error","timestamp":1591710161635,"user_tz":-120,"elapsed":870228,"user":{"displayName":"Michał Zendran","photoUrl":"https://lh4.googleusercontent.com/-EdIWItliYF8/AAAAAAAAAAI/AAAAAAAAGOc/mlyaLViln_w/s64/photo.jpg","userId":"03602240860466142034"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1xXOoQp5yC974FFH6cQdWJBchla7T0NqZ"}},"source":["# Load model\n","encoder_path = './drive/My Drive/ColabStuff/VAEGAN/encoder_200.h5'\n","loaded_enocder = tf.keras.models.load_model(encoder_path, compile=False)\n","\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","def encoder_loss(original, faked):\n","    tf.debugging.check_numerics(faked, \"Nan or Inf in input occurred\", name=None)\n","    return mean_squared_error(original, faked)\n","\n","def kl_loss(mean_mu, log_var):\n","    kl_loss =  -0.5 * tf.reduce_sum(1 + log_var - tf.square(mean_mu) - tf.exp(log_var), axis = 1)\n","    return kl_loss\n","\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","mean_squared_error = tf.keras.losses.MeanSquaredError()\n","\n","# Load and prepare data\n","npz_path = './drive/My Drive/ColabStuff/DiCaprioToDowneyJr_Big_VAE.npz'\n","(x_train, y_train), (x_test, y_test) = load_data(npz_path)\n","\n","value_range = (-1.0, 1.0)\n","print(\"Imported data shape: \" + str(x_train.shape))\n","\n","(x_train, y_train), (x_test, y_test) = preprocess_data(((x_train, y_train), (x_test, y_test)), value_range)\n","\n","# imported_data_sample = get_random_batch(x_train, 16)\n","# show_images(imported_data_sample, value_range)\n","\n","(x_train, y_train), (x_test, y_test) = add_mirrored_data(((x_train, y_train), (x_test, y_test)))\n","\n","# imported_data_sample = get_random_batch(x_train, 16)\n","# show_images(imported_data_sample, value_range)\n","\n","save_path = './drive/My Drive/ColabStuff/VAEGAN/'\n","\n","train_x = np.concatenate((x_train, x_test))\n","train_y = np.concatenate((y_train, y_test))\n","train_z = np.concatenate((train_x, train_y))\n","\n","batch_size = 32\n","code_size = 200\n","epochs_number_1 =20000\n","epochs_number_2 =500\n","LEARNING_RATE = 0.0005\n","IMG_SHAPE = train_x.shape[1:]\n","\n","gen_loss_plot = []\n","disc_loss_plot = []\n","plot_iteration = []\n","\n","discriminator = build_discriminator(IMG_SHAPE)\n","generator = build_generator(IMG_SHAPE, code_size)\n","\n","generator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n","discriminator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n","\n","for i in range(1, epochs_number_1 + 1):\n","  images_train = get_random_batch(train_y, batch_size)\n","  latent = loaded_enocder.predict(images_train)\n","\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","      generated_images = generator(latent, training=True)\n","\n","      real_output = discriminator(images_train, training=True)\n","      fake_output = discriminator(generated_images, training=True)\n","\n","      enc_loss = encoder_loss(images_train, generated_images) * 100\n","      gen_loss = generator_loss(fake_output) + enc_loss\n","      disc_loss = discriminator_loss(real_output, fake_output)\n","  \n","  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","  gen_loss_plot.append(gen_loss)\n","  disc_loss_plot.append(disc_loss)\n","  plot_iteration.append(i)\n","  \n","  if (i != 1) and (i % 10 == 0):\n","    print('setp: ' + str(i) + \", gen_loss: \" + str(gen_loss.numpy()) + \", disc_loss: \" + str(disc_loss.numpy()))\n","\n","  if (i != 1)and (i % 100 == 0):\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, gen_loss_plot, \"C1\", label=\"generator loss\")\n","      ax1.legend()\n","\n","      ax1.plot(plot_iteration, disc_loss_plot, \"C2\", label=\"discriminator loss\")\n","      ax1.legend()\n","\n","      plt.show()\n","  \n","  if (i != 1) and (i % 100 == 0):\n","    test_sample_X = get_random_batch(train_x, 4)\n","    test_sample_Y = get_random_batch(train_y, 4)\n","\n","    visualize(test_sample_X, test_sample_Y, loaded_enocder, generator, value_range)\n","\n","  if (i != 1) and (i % 100 == 0):\n","    generator.save(save_path + \"generator_Y_\" + str(i) + \".h5\")\n","    discriminator.save(save_path + \"discriminator_Y_\" + str(i) + \".h5\")\n","\n","print(\"Program finished successfully!\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}