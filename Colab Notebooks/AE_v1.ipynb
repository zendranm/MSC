{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AE_v1.ipynb","provenance":[],"collapsed_sections":["CzPpqjB4XJMW","GQdysqlVB11L","uyYCnVSULd2b"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CzPpqjB4XJMW","colab_type":"text"},"source":["# Drive and GPU Connection"]},{"cell_type":"markdown","metadata":{"id":"7yDVpNGie-yB","colab_type":"text"},"source":["Run to connect with google drive to access data"]},{"cell_type":"code","metadata":{"id":"_CUcVRIaXOe6","colab_type":"code","outputId":"671c584f-e576-469a-cc18-a5472919600e","executionInfo":{"status":"ok","timestamp":1591611448234,"user_tz":-120,"elapsed":20550,"user":{"displayName":"Michał Zendran","photoUrl":"https://lh4.googleusercontent.com/-EdIWItliYF8/AAAAAAAAAAI/AAAAAAAAGOc/mlyaLViln_w/s64/photo.jpg","userId":"03602240860466142034"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4bbPYYBUgTmz","colab_type":"text"},"source":["Verify GPU"]},{"cell_type":"code","metadata":{"id":"YYRUNyj5gU8C","colab_type":"code","colab":{}},"source":["import tensorflow as tf \n","tf.test.gpu_device_name()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQdysqlVB11L","colab_type":"text"},"source":["# Data preprocess and visualization"]},{"cell_type":"code","metadata":{"id":"hvSA0RTJBxVF","colab_type":"code","outputId":"50810251-01db-44d9-df6b-c7651d0305c6","executionInfo":{"status":"ok","timestamp":1591611540207,"user_tz":-120,"elapsed":2606,"user":{"displayName":"Michał Zendran","photoUrl":"https://lh4.googleusercontent.com/-EdIWItliYF8/AAAAAAAAAAI/AAAAAAAAGOc/mlyaLViln_w/s64/photo.jpg","userId":"03602240860466142034"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras import Sequential, Input, Model\n","from tensorflow.keras.layers import Dense, Activation, Flatten, Reshape, Lambda\n","from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, MaxPooling2D\n","from tensorflow.keras.layers import LeakyReLU, Dropout\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from keras import backend as K\n","\n","def load_data(filename):\n","    with np.load(filename) as f:\n","        x_train, x_test = f['arr_0'], f['arr_1']\n","        y_train, y_test = f['arr_2'], f['arr_3']\n","        return (x_train, y_train), (x_test, y_test)\n","\n","def preprocess_data(data, feature_range=(-1.0, 1.0)):\n","    (x_train, y_train), (x_test, y_test) = data\n","    min, max = feature_range\n","\n","    x_train = x_train.reshape(-1, 160, 160, 3).astype(np.float32)\n","    x_train = x_train/255.0\n","    x_train = x_train * (max - min) + min\n","\n","    x_test = x_test.reshape(-1, 160, 160, 3).astype(np.float32)\n","    x_test = x_test/255.0\n","    x_test = x_test * (max - min) + min\n","\n","    y_train = y_train.reshape(-1, 160, 160, 3).astype(np.float32)\n","    y_train = y_train/255.0\n","    y_train = y_train * (max - min) + min\n","\n","    y_test = y_test.reshape(-1, 160, 160, 3).astype(np.float32)\n","    y_test = y_test/255.0\n","    y_test = y_test * (max - min) + min\n","\n","    print(\"Values after preprocessing, min: \" + str(x_train.min()) + \", max: \" + str(x_train.max()))\n","    return (x_train, y_train), (x_test, y_test)\n","\n","def add_mirrored_data(data):\n","    (x_train, y_train), (x_test, y_test) = data\n","\n","    x_train_mirrored = np.flip(x_train, 2)\n","    y_train_mirrored = np.flip(y_train, 2)\n","    x_test_mirrored = np.flip(x_test, 2)\n","    y_test_mirrored = np.flip(y_test, 2)\n","\n","    x_train = np.concatenate((x_train, x_train_mirrored))\n","    y_train = np.concatenate((y_train, y_train_mirrored))\n","    x_test = np.concatenate((x_test, x_test_mirrored))\n","    y_test = np.concatenate((y_test, y_test_mirrored))\n","\n","    return (x_train, y_train), (x_test, y_test)\n","\n","def get_random_batch(data, batch_size):\n","    idx = np.random.choice(data.shape[0], batch_size)\n","    batch = data[idx].reshape((batch_size, 160, 160, 3))\n","    return batch\n","\n","def show_images(images, feature_range=(-1.0, 1.0)):\n","    min, max = feature_range\n","    plt.figure(figsize=(10,10))\n","    \n","    for im in range(images.shape[0]):\n","      plt.subplot(4, 4, im+1)\n","      image = images[im, :, :, :]\n","      image = np.reshape(image, [160,160,3])\n","      image = (image - min) / (max - min)\n","      plt.imshow(image)\n","      plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","def visualize(sample_X, sample_Y, encoder, decoder, feature_range=(-1.0, 1.0)):\n","    min, max = feature_range\n","    count = sample_X.shape[0]\n","\n","    reconstructed_X = []\n","    reconstructed_Y = []\n","    accumulated = []\n","\n","    for n in range(count):\n","        code = encoder.predict(sample_X[n][None])[0]\n","        reco = decoder.predict(code[None])[0]\n","\n","        reconstructed_X.append(reco)\n","\n","        code = encoder.predict(sample_Y[n][None])[0]\n","        reco = decoder.predict(code[None])[0]\n","\n","        reconstructed_Y.append(reco)\n","\n","    for i in range(4):\n","        accumulated.append(sample_X[i])\n","        accumulated.append(reconstructed_X[i])\n","        accumulated.append(sample_Y[i])\n","        accumulated.append(reconstructed_Y[i])\n","\n","    count = len(accumulated)\n","\n","    plt.figure(figsize=(8,8))\n","\n","    for i in range(count):\n","        plt.subplot(4, 4, i + 1)\n","        image = (accumulated[i] - min) / (max - min)\n","        plt.imshow(image)\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"uyYCnVSULd2b","colab_type":"text"},"source":["# AE networks"]},{"cell_type":"code","metadata":{"id":"7EIX3LfuLcmx","colab_type":"code","colab":{}},"source":["def build_encoder(img_shape, code_size):\n","    encoder_input = Input(shape=img_shape, name='encoder_input')\n","\n","    x = encoder_input\n","\n","    x = Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), padding='same', name='encoder_conv_1')(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding='same', name='encoder_conv_2')(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(filters=128, kernel_size=(4, 4), strides=(2, 2), padding='same', name='encoder_conv_3')(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(filters=256, kernel_size=(4, 4), strides=(2, 2), padding='same', name='encoder_conv_4')(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Flatten()(x)\n","\n","    encoder_output = Dense(code_size, name='encoder_output')(x)\n","\n","    return encoder_input, encoder_output, Model(encoder_input, encoder_output)\n","\n","def build_decoder(img_shape, code_size):\n","    size1 = int(img_shape[0]/16) # 8 is 2^Conv2D in encoder\n","    size2 = int(img_shape[1]/16)\n","\n","    decoder_input = Input(shape=(int(code_size,)), name='decoder_input')\n","\n","    x = decoder_input\n","\n","    x = Dense(units=(size1*size2*256))(x)\n","    x = Reshape(target_shape=(size1, size2, 256))(x)\n","    \n","    x = Conv2DTranspose(filters=128, kernel_size=(3, 3), strides=(2, 2), padding='same', name='decoder_conv_1')(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='same', name='decoder_conv_2')(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=(2, 2), padding='same', name='decoder_conv_3')(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2DTranspose(filters=3, kernel_size=(3, 3), strides=(2, 2), padding='same', name='decoder_conv_4')(x)\n","    x = Activation('tanh')(x)\n","\n","    decoder_output = x\n","\n","    return decoder_input, decoder_output, Model(decoder_input, decoder_output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nO6s4S5W3NO","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"OyD_5tRabmiC","colab_type":"code","outputId":"4e459f18-3f22-4cca-b612-e282f34af3cc","executionInfo":{"status":"error","timestamp":1591611718659,"user_tz":-120,"elapsed":140418,"user":{"displayName":"Michał Zendran","photoUrl":"https://lh4.googleusercontent.com/-EdIWItliYF8/AAAAAAAAAAI/AAAAAAAAGOc/mlyaLViln_w/s64/photo.jpg","userId":"03602240860466142034"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1PrvYGWp_1VuR27K9DT4Ln8iR_K4m2fBX"}},"source":["tf.compat.v1.disable_eager_execution()\n","# Load and prepare data\n","npz_path = './drive/My Drive/ColabStuff/DiCaprioToDowneyJr_Big_VAE.npz'\n","(x_train, y_train), (x_test, y_test) = load_data(npz_path)\n","\n","value_range = (-1.0, 1.0)\n","print(\"Imported data shape: \" + str(x_train.shape))\n","\n","(x_train, y_train), (x_test, y_test) = preprocess_data(((x_train, y_train), (x_test, y_test)), value_range)\n","\n","# imported_data_sample = get_random_batch(x_train, 16)\n","# show_images(imported_data_sample, value_range)\n","\n","(x_train, y_train), (x_test, y_test) = add_mirrored_data(((x_train, y_train), (x_test, y_test)))\n","\n","# imported_data_sample = get_random_batch(x_train, 16)\n","# show_images(imported_data_sample, value_range)\n","\n","save_path = './drive/My Drive/ColabStuff/AE/'\n","\n","train_x = np.concatenate((x_train, x_test))\n","train_y = np.concatenate((y_train, y_test))\n","\n","# # # # # # AE training 1 # # # # # #\n","\n","IMG_SHAPE = x_train.shape[1:]\n","code_size = 200\n","epochs_number_1 =150\n","epochs_number_2 =500\n","LEARNING_RATE = 0.0005\n","\n","ae_encoder_input, ae_encoder_output, ae_encoder = build_encoder(IMG_SHAPE, code_size)\n","print(ae_encoder.summary())\n","\n","ae_decoder_input, ae_decoder_output, ae_decoder = build_decoder(IMG_SHAPE, code_size)\n","print(ae_decoder.summary())\n","\n","ae_input = ae_encoder_input\n","ae_output = ae_decoder(ae_encoder_output)\n","ae_model = Model(ae_input, ae_output)\n","\n","\n","def r_loss(y_true, y_pred):\n","    r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n","    return r_loss\n","  \n","adam_optimizer = Adam(lr = LEARNING_RATE)\n","\n","ae_model.compile(optimizer=adam_optimizer, loss = r_loss, metrics = ['accuracy'])\n","\n","batch_size = 128\n","\n","loss_dis = []\n","acc_dis = []\n","plot_iteration = []\n","\n","for i in range(1, epochs_number_1 + 1):\n","    images_train_x = get_random_batch(train_x, batch_size)\n","    images_train_y = get_random_batch(train_y, batch_size)\n","\n","    train_z = np.concatenate((images_train_x, images_train_y))\n","\n","    d_stats = ae_model.train_on_batch(x= train_z, y= train_z)\n","    loss_dis.append(d_stats[0])\n","    acc_dis.append(d_stats[1])\n","    plot_iteration.append(i)\n","\n","    if (i != 1)and (i % 50 == 0):\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, loss_dis, label=\"loss\")\n","      ax1.legend()\n","\n","      plt.show()\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, acc_dis, \"C1\", label=\"accuracy\")\n","      ax1.legend()\n","\n","      plt.show()\n","          \n","    if (i != 1) and (i % 50 == 0):\n","\n","      test_sample_X = get_random_batch(train_x, 4)\n","      test_sample_Y = get_random_batch(train_y, 4)\n","\n","      visualize(test_sample_X, test_sample_Y, ae_encoder, ae_decoder, value_range)\n","\n","ae_encoder.save(save_path + \"encoder_\" + str(i) + \".h5\")\n","ae_decoder.save(save_path + \"decoder_XY_\" + str(i) + \".h5\")\n","\n","layer_number = 0\n","for layer in ae_encoder.layers:\n","  layer_number += 1\n","  print(\"Layer: \" + str(layer_number))\n","  layer.trainable = False\n","\n","# # # # # # AE end training 1 # # # # # #\n","\n","# # # # # # AE training 2 # # # # # #\n","\n","ae2_decoder_input, ae2_decoder_output, ae2_decoder = build_decoder(IMG_SHAPE, code_size)\n","print(ae2_decoder.summary())\n","\n","ae2_output = ae2_decoder(ae_encoder_output)\n","ae2_model = Model(ae_input, ae2_output)\n","\n","ae2_model.compile(optimizer=adam_optimizer, loss = r_loss, metrics = ['accuracy'])\n","\n","loss_dis = []\n","acc_dis = []\n","plot_iteration = []\n","\n","for i in range(1, epochs_number_2 + 1):\n","    images_train_y = get_random_batch(train_y, batch_size)\n","\n","    d_stats = ae2_model.train_on_batch(x= images_train_y, y= images_train_y)\n","    loss_dis.append(d_stats[0])\n","    acc_dis.append(d_stats[1])\n","    plot_iteration.append(i)\n","\n","    if (i !=1)and (i % 50 == 0):\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, loss_dis, label=\"loss discriminator\")\n","      ax1.legend()\n","\n","      plt.show()\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, acc_dis, \"C1\", label=\"acc discriminator\")\n","      ax1.legend()\n","\n","      plt.show()\n","          \n","    if (i != 1) and (i % 50 == 0):\n","\n","      test_sample_X = get_random_batch(train_x, 4)\n","      test_sample_Y = get_random_batch(train_y, 4)\n","\n","      visualize(test_sample_X, test_sample_Y, ae_encoder, ae2_decoder, value_range)\n","    \n","    if (i > 99) and (i % 50 == 0):\n","      ae2_decoder.save(save_path + \"decoder_Y_\" + str(i) + \".h5\")\n","\n","# # # # # # # AE end training 2 # # # # # #\n","\n","# # # # # # AE training 3 # # # # # #\n","\n","ae3_decoder_input, ae3_decoder_output, ae3_decoder = build_decoder(IMG_SHAPE, code_size)\n","print(ae3_decoder.summary())\n","\n","ae3_output = ae3_decoder(ae_encoder_output)\n","ae3_model = Model(ae_input, ae3_output)\n","\n","ae3_model.compile(optimizer=adam_optimizer, loss = r_loss, metrics = ['accuracy'])\n","\n","loss_dis = []\n","acc_dis = []\n","plot_iteration = []\n","\n","for i in range(1, epochs_number_2 + 1):\n","    images_train_x = get_random_batch(train_x, batch_size)\n","\n","    d_stats = ae3_model.train_on_batch(x= images_train_x, y= images_train_x)\n","    loss_dis.append(d_stats[0])\n","    acc_dis.append(d_stats[1])\n","    plot_iteration.append(i)\n","\n","    if (i !=1)and (i % 50 == 0):\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, loss_dis, label=\"loss discriminator\")\n","      ax1.legend()\n","\n","      plt.show()\n","\n","      (fig, (ax1)) = plt.subplots(1)\n","      fig.set_size_inches(8, 8)\n","\n","      ax1.plot(plot_iteration, acc_dis, \"C1\", label=\"acc discriminator\")\n","      ax1.legend()\n","\n","      plt.show()\n","          \n","    if (i != 1) and (i % 50 == 0):\n","\n","      test_sample_X = get_random_batch(train_x, 4)\n","      test_sample_Y = get_random_batch(train_y, 4)\n","\n","      visualize(test_sample_X, test_sample_Y, ae_encoder, ae3_decoder, value_range)\n","    \n","    if (i > 99) and (i % 50 == 0):\n","      ae3_decoder.save(save_path + \"decoder_X_\" + str(i) + \".h5\")\n","\n","# # # # # # # AE end training 3 # # # # # #\n","\n","\n","print(\"Program finished successfully!\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}