\chapter{Deepfake methods}
\section{Autoencoder}
\label{Autoencoder}
Autoencoders are the most basic approach to the problem of deepfake generation. In fact, all mentioned methods, except for CycleGAN, are just different variations of this idea. Autoencoder is a type of artificial neural network that learns to reproduce given input in an unsupervised manner. The goal is to train functions \(A: \mathbb{R}^n \to \mathbb{R}^p\) (encoder) and \(B: \mathbb{R}^p \to \mathbb{R}^n\) (decoder) to satisfy condition given in equation \ref{eq:autoencoder_condition} as described in \cite{autoencoders_bib},
%
\begin{equation}
\label{eq:autoencoder_condition}
arg \, min_{A,B} E[\Delta(x,B \circ A(x))]
\end{equation}
%
where \(E\) -- expectation over the distribution of \(x\) and \(\Delta\) -- reconstruction loss function, which measures the distance between given input and the output of the decoder. General idea of autoencoder model is illustrated in figure \ref{fig:autoencoder_general_idea}. Typically, architecture of autoencoder consists not only of input and output layers, as this would result in simple coping pixels from the input to the output of the network, but also contains single or multiple hidden layers in between, with the number of neurons lesser than the number of pixels in the input image. Such structure causes bottleneck effect and creates so-called compressed representation at the output of the encoder part, known also as ``feature map'' or in case of deepfake ``latent face''. Such compression causes feature map to preserve only information most relevant for later reconstruction and gets rid of unnecessary data.

\begin{figure}[H]
\includegraphics[width=12cm] {autoencoder_general_idea.png}
\centering
\caption{General idea of autoencoder}
\label{fig:autoencoder_general_idea}
\end{figure}

Generating deepfakes using autoencoders approach consists of three major steps. Described process is illustrated in figure \ref{fig:deepfake_steps}. Let us assume that \(X\) is a set of face images of person \(x\) and \(Y\) is a set of face images of person \(y\). Firstly, as shown in figure \ref{subfig:deepfake_steps_a}, an encoder is trained to produce feature maps for images of faces from both classes. Afterwards, two decoders are trained separately to reproduce original images from latent faces generated by pre-trained encoder, as presented in figure \ref{subfig:deepfake_steps_b}. Finally, decoders are switched to produce images from one class based on feature maps from the other class, which was illustrated in figure \ref{subfig:deepfake_steps_c}.

\begin{figure}[H]
\centering
\begin{subfigure}{12cm}
\includegraphics[width=\textwidth]{deepfake_idea_1.png}
\caption{Training autoencoder}
\label{subfig:deepfake_steps_a}
\end{subfigure}

\begin{subfigure}{12cm}
\includegraphics[width=\textwidth]{deepfake_idea_2.png}
\caption{Training decoders X and Y}
\label{subfig:deepfake_steps_b}
\end{subfigure}

\begin{subfigure}{12cm}
\includegraphics[width=\textwidth]{deepfake_idea_3.png}
\caption{Generating X from Y and Y from X}
\label{subfig:deepfake_steps_c}
\end{subfigure}

\caption{Three steps of generating deepfakes}
\label{fig:deepfake_steps}
\end{figure}

\newpage
\section{Variational autoencoder}
Unlike basic autoencoders, which purpose is to reproduce given input by ``memorizing'' it, variational autoencoders aim to produce latent vectors (feature maps) that follow a unit Gaussian distribution \cite{variational_bayes_bib}. Such modification allows to generate new images by sampling a latent vector from the unit Gaussian distribution and pass it as an input of the decoder network. The goal in VAE model training is to optimize the loss function given in equation \ref{eq:vae_loss} \cite{vae_loss_bib}. First component of this function is a reconstruction loss \(\mathcal{L}_{rec}\), which, by comparing pixels from image \(x\), encoded to a latent vector \(z = Encoder(x) \sim q(z|x)\), and reconstructed image \(\bar{x} = Decoder(z) \sim p(x|z)\), measures expected marginal log-likelihood of the observations in \(x\). Second component, so called Kullback–Leibler divergence \cite{kl_divergence_bib}, measures the extent to which one probability distribution differs from another. Applying KL divergence to VAE loss function allows to control distribution of the latent vector \(z\).
%
\begin{equation}
\label{eq:vae_loss}
\mathcal{L}_{vae} = \mathcal{L}_{rec} + \mathcal{L}_{kl}
\end{equation}
%
\begin{equation}
\label{eq:rec_loss}
\mathcal{L}_{rec} = -E_{q(z|x)}[\log{p(x|z)}]
\end{equation}
%
\begin{equation}
\label{eq:kl_loss}
\mathcal{L}_{kl} = D_{kl}(q(z|x)||p(z|x))
\end{equation}

Presence of KL divergence in VAE loss function generates problem with applying backpropagation to the encoder network, as decoder randomly samples from latent vector \(z\) and backpropagation cannot flow through random node as in figure \ref{subfig:before_reparameterization}. As a solution, a ``reparameterization trick'' is introduced \cite{reparameterization_trick_bib}. This method maps the output of an encoder to separate vectors, mean \(\mu\) and standard deviation \(\sigma\), instead of a z-dimensional latent vector. Such representation allows to calculate \(z\) in a following way:
%
\begin{equation}
\label{eq:rec_loss}
z = \mu + \sigma\epsilon
\end{equation}
%
where \(\epsilon\) is sampled from a multivariate standard normal distribution (\(Normal(0,1)\)) and therefore acts as a stochastic component. Such reparameterization allows to run backpropagation process as now latent vector is not purely random, but consists of deterministic and stochastic nodes as illustrated in figure \ref{subfig:after_reparameterization}.

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{before_reparameterization.png}
  \caption{Nodes before reparameterization}
  \label{subfig:before_reparameterization}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{after_reparameterization.png}
  \caption{Nodes after reparameterization}
  \label{subfig:after_reparameterization}
\end{subfigure}
\caption{Reparameterization trick}
\label{fig:reparameterization_trick}
\end{figure}

By applying reparameterization trick to equation \ref{eq:kl_loss}, Kullback–Leibler divergence takes following form \cite{kl_divergence_loss_bib}:
%
\begin{equation}
\label{eq:final_kl_loss}
\mathcal{L}_{kl} = -\frac{1}{2}\sum_{i=0}^{z}(1 + \log(\sigma_i^2) - \mu_i^2 -\sigma_i^2)
\end{equation}

Generating deepfakes using variational autoencoders approach is based on the same algorithm as autoencoders approach illustrated in figure \ref{fig:deepfake_steps}. The only difference between those two methods is the way the latent face (latent vector) is obtained.

\section{VAE-GAN}
Idea behind deepfake generated by GAN actually ''VAE-GAN''.

\section{CycleGAN}
Describe what is it, what it consists of, what are its applications, why I thought it should work for deepfake. Explain how it works exactly. Show learning process and results (good ones: horses to zebras and bad ones: face to face). Idea behind deepfake generated by CycleGAN. Explain why I'm assuming it should it work?